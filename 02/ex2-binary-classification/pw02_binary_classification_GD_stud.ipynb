{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef52aa8-c74f-450e-ac70-01905d511493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98670d0a-08fb-4645-887b-4eb5d02cff00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Binary Classification\n",
    "\n",
    "Here, we use a tabular dataset from kaggle (https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset) with features on patients physical spine details possibly suited for classifying whether the person is 'abnormal' or 'normal' - possibly suffers back pain or not.   \n",
    "\n",
    "We here just want to see how the training works with logistic regression (binary case). We set aside a proper handling of the learning experiment by splitting the data into a train and test partition (in general we would even have a validation partition). We focus here on making the system learn something. \n",
    "\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275b5e04-7dbc-461d-9e63-c4e1c3852c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>Col11</th>\n",
       "      <th>Col12</th>\n",
       "      <th>Class_att</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.027817</td>\n",
       "      <td>22.552586</td>\n",
       "      <td>39.609117</td>\n",
       "      <td>40.475232</td>\n",
       "      <td>98.672917</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>0.744503</td>\n",
       "      <td>12.5661</td>\n",
       "      <td>14.5386</td>\n",
       "      <td>15.30468</td>\n",
       "      <td>-28.658501</td>\n",
       "      <td>43.5123</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.056951</td>\n",
       "      <td>10.060991</td>\n",
       "      <td>25.015378</td>\n",
       "      <td>28.995960</td>\n",
       "      <td>114.405425</td>\n",
       "      <td>4.564259</td>\n",
       "      <td>0.415186</td>\n",
       "      <td>12.8874</td>\n",
       "      <td>17.5323</td>\n",
       "      <td>16.78486</td>\n",
       "      <td>-25.530607</td>\n",
       "      <td>16.1102</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.832021</td>\n",
       "      <td>22.218482</td>\n",
       "      <td>50.092194</td>\n",
       "      <td>46.613539</td>\n",
       "      <td>105.985135</td>\n",
       "      <td>-3.530317</td>\n",
       "      <td>0.474889</td>\n",
       "      <td>26.8343</td>\n",
       "      <td>17.4861</td>\n",
       "      <td>16.65897</td>\n",
       "      <td>-29.031888</td>\n",
       "      <td>19.2221</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.297008</td>\n",
       "      <td>24.652878</td>\n",
       "      <td>44.311238</td>\n",
       "      <td>44.644130</td>\n",
       "      <td>101.868495</td>\n",
       "      <td>11.211523</td>\n",
       "      <td>0.369345</td>\n",
       "      <td>23.5603</td>\n",
       "      <td>12.7074</td>\n",
       "      <td>11.42447</td>\n",
       "      <td>-30.470246</td>\n",
       "      <td>18.8329</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.712859</td>\n",
       "      <td>9.652075</td>\n",
       "      <td>28.317406</td>\n",
       "      <td>40.060784</td>\n",
       "      <td>108.168725</td>\n",
       "      <td>7.918501</td>\n",
       "      <td>0.543360</td>\n",
       "      <td>35.4940</td>\n",
       "      <td>15.9546</td>\n",
       "      <td>8.87237</td>\n",
       "      <td>-16.378376</td>\n",
       "      <td>24.9171</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col1       Col2       Col3       Col4        Col5       Col6  \\\n",
       "0  63.027817  22.552586  39.609117  40.475232   98.672917  -0.254400   \n",
       "1  39.056951  10.060991  25.015378  28.995960  114.405425   4.564259   \n",
       "2  68.832021  22.218482  50.092194  46.613539  105.985135  -3.530317   \n",
       "3  69.297008  24.652878  44.311238  44.644130  101.868495  11.211523   \n",
       "4  49.712859   9.652075  28.317406  40.060784  108.168725   7.918501   \n",
       "\n",
       "       Col7     Col8     Col9     Col10      Col11    Col12 Class_att  \n",
       "0  0.744503  12.5661  14.5386  15.30468 -28.658501  43.5123  Abnormal  \n",
       "1  0.415186  12.8874  17.5323  16.78486 -25.530607  16.1102  Abnormal  \n",
       "2  0.474889  26.8343  17.4861  16.65897 -29.031888  19.2221  Abnormal  \n",
       "3  0.369345  23.5603  12.7074  11.42447 -30.470246  18.8329  Abnormal  \n",
       "4  0.543360  35.4940  15.9546   8.87237 -16.378376  24.9171  Abnormal  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Dataset_spine.csv\") # possibly modify!\n",
    "df = df.drop(columns=['Unnamed: 13'])\n",
    "N  = df.shape[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb0957-0a55-4437-9076-dd1cad0354c0",
   "metadata": {},
   "source": [
    "### Normalization and Turning into Torch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418ab33a-b905-4b9c-aff9-aa0e48bf9a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([310, 12]) torch.Size([310, 1])\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.from_numpy(df.values[:,0:-1].astype(np.float64))\n",
    "X = (x0-torch.mean(x0, dim=0))/torch.std(x0,dim=0)\n",
    "Y = torch.tensor(('Abnormal'==df.values[:,-1])).int().reshape(-1,1)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95eefb-3767-4884-a799-7617e9428a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Binary) Logistic Regression\n",
    "\n",
    "Data:  $\\,\\qquad X = \\left(\\begin{array}{cccc} 1 & X_{11} & \\dots & X_{1n} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & X_{N1} & \\dots & X_{Nn}\\end{array}\\right)\\qquad$ and $\\qquad Y = \\left(\\begin{array}{c} Y_{1} \\\\ \\vdots \\\\ Y_{N} \\end{array}\\right)$\n",
    "\n",
    "Model: $\\qquad\\hat{Y}(X;W) = \\sigma\\left(X W^\\intercal\\right) \\qquad$ where $\\qquad W = \\left(\\begin{array}{c} W_0 \\\\ W_1 \\\\ \\vdots \\\\ W_n \\end{array}\\right)$\n",
    "\n",
    "The model outputs the probability of observing in a sample $x$ a '1' (Abnormal).\n",
    "\n",
    "Cost:  $\\,\\qquad C(W) = -\\frac{1}{N}\\sum_j \\left(Y_j\\log(\\hat{Y}_j(X;W)) + (1-Y_j)\\log(1-\\hat{Y}_j(X;W))\\right)$\n",
    "\n",
    "__Remark:__ Note that the logarithm diverges at arguments approaching 0. Make sure that you don't run into numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79210499-8d67-4f08-9da3-91a9b2f93f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compose torch tensors X of shape (N,13) by inserting a column with 1's as first column  \n",
    "X = torch.cat((torch.ones(N,1),X), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf5c8e",
   "metadata": {},
   "source": [
    "Assuming we use sigmoid activation function: \n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5053b3b8-4e77-45c7-8a57-e9abe98c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement methods for predicting the probability of having label 0 or 1 (W with shape (1,13))\n",
    "def predict(X,W):\n",
    "    # YOUR CODE (START)\n",
    "    z = X@W.T\n",
    "    return torch.div(1, 1+torch.exp(-z))\n",
    "    # YOUR CODE (END)\n",
    "\n",
    "def cost(X,Y,W):\n",
    "    # YOUR CODE (START)\n",
    "    Yhat = predict(X,W)\n",
    "    -1/len(X) * torch.sum(Y*torch.log(Yhat)+(1-Y)*torch.log(1-Yhat))\n",
    "    # YOUR CODE (END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f342c9",
   "metadata": {},
   "source": [
    "Cost:  $\\,\\qquad C(W) = -\\frac{1}{N}\\sum_j \\left(Y_j\\log(\\hat{Y}_j(X;W)) + (1-Y_j)\\log(1-\\hat{Y}_j(X;W))\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b73745b8-c2f5-4150-a8f7-2e49f72f3566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_cost(X,Y,W):\n",
    "    # YOUR CODE (START)\n",
    "    Yhat = predict(X, W)\n",
    "    return torch.div(X.T@torch.sub(Yhat, Y), len(X)).view(1,13) # ?\n",
    "    # YOUR CODE (END)\n",
    "    \n",
    "def accuracy(Y,Yhat):\n",
    "    # YOUR CODE (START)\n",
    "    Yhat_cat = torch.where(Yhat > 0.5, 1, 0)\n",
    "    return torch.div((Y == Yhat_cat).sum(), len(Y))\n",
    "    # YOUR CODE (END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79340e-41e2-4b8a-a1c3-f94d0e00c69e",
   "metadata": {},
   "source": [
    "Just for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dd82c26-6db7-43ec-84c5-2e1a9c998d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1085], dtype=torch.float64)\n",
      "None\n",
      "tensor([[-0.0024, -0.1320, -0.1823, -0.0625, -0.0335,  0.1402, -0.0768, -0.1512,\n",
      "          0.0191,  0.1054,  0.0282,  0.0535,  0.0149]], dtype=torch.float64)\n",
      "tensor(0.6613)\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((1,13), dtype=torch.double)\n",
    "print(predict(X[0],W))\n",
    "print(cost(X,Y,W))\n",
    "print(gradient_cost(X,Y,W))\n",
    "print(accuracy(Y,predict(X,W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503ca14",
   "metadata": {},
   "source": [
    "* tensor([0.0185], dtype=torch.float64)\n",
    "* tensor(1.8740, dtype=torch.float64)\n",
    "* tensor([[-0.0762, -0.1208, -0.0390, -0.1175, -0.1261,  0.3880, -0.1477, -0.0384, 0.1195,  0.1263, -0.1280,  0.0705,  0.1423]], dtype=torch.float64)\n",
    "* tensor(0.4710)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc000103-2e8d-4570-a9a6-c4aa062928cb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76f0b32a-fd22-4a94-b923-8714538a0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (max,end): 0.867742, 0.854839\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m accs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(accs)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Accuracy (max,end): \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(np\u001b[38;5;241m.\u001b[39mmax(accs), accs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Cost (end): \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m\u001b[43mcosts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m())\n\u001b[0;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(nepochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),costs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "# adjust if needed\n",
    "nepochs = 1000\n",
    "lr = 1.0\n",
    "\n",
    "## initial parameter\n",
    "W = torch.randn((1,13), dtype=torch.double)\n",
    "\n",
    "# track the costs\n",
    "costs = [cost(X,Y,W)]\n",
    "accs = [accuracy(Y,predict(X,W))]\n",
    "\n",
    "# YOUR CODE (START)\n",
    "\n",
    "# loop over the epochs: update parameter values, compute the cost and add it to the costs list\n",
    "for epoch in range(nepochs):\n",
    "    dw = gradient_cost(X, Y, W)\n",
    "    W = torch.sub(W, lr * dw)\n",
    "    costs.append(cost(X,Y,W))\n",
    "    accs.append(accuracy(Y,predict(X,W)))\n",
    "\n",
    "# YOUR CODE (END)\n",
    "    \n",
    "# some output\n",
    "accs = np.array(accs)\n",
    "\n",
    "print(\"Training Accuracy (max,end): %f, %f\"%(np.max(accs), accs[-1]))\n",
    "print(\"Training Cost (end): %f\"%costs[-1].item())\n",
    "plt.figure(1)\n",
    "plt.plot(range(nepochs+1),costs)\n",
    "plt.figure(2)\n",
    "plt.plot(range(nepochs+1),accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc7cf8-ffbd-4d78-a015-7cf2c540ad4c",
   "metadata": {},
   "source": [
    "### Different Learning Rates\n",
    "\n",
    "Play with different learning rates: Explore for what learning rates \n",
    "- the learning is most efficient\n",
    "- the learning yet works\n",
    "- the learning does not work anymore (learning rate too large)\n",
    "\n",
    "Explain the different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9e926",
   "metadata": {},
   "source": [
    "* the learning is most efficient\n",
    " * Learning seems to be most efficient around a learning rate of 0.5\n",
    "* the learning yet works\n",
    " * The learning works around 0.01, but takes more epochs to approach the minimal cost value\n",
    "* the learning does not work anymore (learning rate too large)\n",
    " * Around 8 learning is still working but with very many fluctiations due to the big step size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9f76a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
